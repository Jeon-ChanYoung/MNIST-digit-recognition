{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이토치 구성요소  \n",
    "- torch : 메인, 텐서 등의 다양한 수학 함수 포함  \n",
    "- torch.autograd : 자동 미분 기능을 제공  \n",
    "- torch.nn : 신경망 구축을 위한 데이터 구조나 레이어  \n",
    "- torch.multiprocessing : 병렬처리 기능을 제공  \n",
    "- torch.optim : SGD(Stochastic Gradient Descent(확률적 경사 하강법))를 중심으로 한 파라미터 최적화 알고리즘 제공  \n",
    "- torch.utils : 데이터 조작 등 유틸리티 기능 제공  \n",
    "- torch.onnx : ONNX(Open Neural Network Exchange) 서로 다른 프레임워크 간의 모델을 공유할 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[0.8722, 0.2288],\n",
      "        [0.7902, 0.9704],\n",
      "        [0.8743, 0.7566],\n",
      "        [0.5938, 0.1040]])\n",
      "tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.empty(4,2)\n",
    "y = torch.rand(4,2)\n",
    "z = torch.zeros(4,2, dtype=torch.long)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사용자 입력 텐서\n",
    "x = torch.tensor([3, 2.3])\n",
    "y = torch.ones(2,4, dtype=torch.double)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### view : 텐서의 크기(size)나 모양(shape)을 변경  \n",
    "- 기본적으로 변경 전/후 원소 개수 유지되야함  \n",
    "- -1로 설정하면 자동으로 지정  \n",
    "### item : 텐서에 값이 하나만 존재할 경우 숫자값 반환  \n",
    "### squeeze : 차원 축소  \n",
    "### unsqueeze : 차원 확대 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6024, 0.1715, 0.4026],\n",
      "         [0.1609, 0.8307, 0.0230],\n",
      "         [0.9170, 0.1547, 0.2351]]])\n",
      "tensor([0.6024, 0.1715, 0.4026, 0.1609, 0.8307, 0.0230, 0.9170, 0.1547, 0.2351])\n",
      "tensor([[0.6024, 0.1715, 0.4026],\n",
      "        [0.1609, 0.8307, 0.0230],\n",
      "        [0.9170, 0.1547, 0.2351]])\n",
      "tensor([[0.0692, 0.3262, 0.3707],\n",
      "        [0.8867, 0.4486, 0.7824],\n",
      "        [0.9856, 0.0884, 0.2417]])\n",
      "tensor([[[0.0692, 0.3262, 0.3707],\n",
      "         [0.8867, 0.4486, 0.7824],\n",
      "         [0.9856, 0.0884, 0.2417]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1,3,3)\n",
    "print(x)\n",
    "print(x.view(-1))\n",
    "print(x.squeeze())\n",
    "\n",
    "y = torch.rand(3,3)\n",
    "print(y)\n",
    "print(y.unsqueeze(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd(자동미분)  \n",
    "- torch.autograd는 Tensor의 모든 연산에 대해 자동 미분 제공  \n",
    "- 이는 코드를 어떻게 작성하여 실행하느냐에 따라 역전파가 정의된다는 뜻  \n",
    "- backprop(역전파)를 위해 미분값을 자동으로 계산  \n",
    "\n",
    "requires_grad 속성을 True로 설정하면, 해당 텐서에서 이루어지는 모든 연산들을 추적하기 시작  \n",
    "기록을 추적하는 것을 중단하게하려면, .detach() 를 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3290, -3.0242, -1.7909],\n",
      "        [-0.8860,  3.1785,  0.8304],\n",
      "        [ 3.6662,  4.6558, -2.2917]])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3,3)\n",
    "a = a*3\n",
    "print(a)\n",
    "print(a.requires_grad) #기본값은 False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.requires_grad_(True) #in-place "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, 3, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6., 6., 6.],\n",
      "        [6., 6., 6.],\n",
      "        [6., 6., 6.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x+5\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계산이 완료된 후, .backward()를 호출하면 자동으로 역전파 계산한 다음에 .grad속성에 누적함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(54., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y.sum()\n",
    "print(z)\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grad : data가 거쳐온 layer에 대한 미분값 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], requires_grad=True)\n",
      "tensor([[1.4444, 1.4444, 1.4444],\n",
      "        [1.4444, 1.4444, 1.4444],\n",
      "        [1.4444, 1.4444, 1.4444]])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 자동미분 흐름 예시  \n",
    "- 계산 흐름 : a -> b -> c -> out  \n",
    "- 미분 out / a = ?  \n",
    "- backward()를 통해 a <- b <- c <- out을 계산하면 미분 out / a 값이 a.grad에 채워짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,2,requires_grad=True)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(a.data) \n",
    "print(a.grad) #초기\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "b = a + 2\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9., 9.],\n",
      "        [9., 9.]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c = b ** 2\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = c.sum()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[6., 6.],\n",
      "        [6., 6.]])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(a.data) \n",
    "print(a.grad) # 왜 값이 6이나왔냐면....\n",
    "print(a.grad_fn) #직접 계산한게 없어서 None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "None\n",
      "<AddBackward0 object at 0x00000214027A7B50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JangChanYoung\\AppData\\Local\\Temp\\ipykernel_38456\\3135712180.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(b.grad)\n"
     ]
    }
   ],
   "source": [
    "print(b.data) \n",
    "print(b.grad) \n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9., 9.],\n",
      "        [9., 9.]])\n",
      "None\n",
      "<PowBackward0 object at 0x00000214049CD760>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JangChanYoung\\AppData\\Local\\Temp\\ipykernel_38456\\823509336.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(c.grad)\n"
     ]
    }
   ],
   "source": [
    "print(c.data) \n",
    "print(c.grad) \n",
    "print(c.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36.)\n",
      "None\n",
      "<SumBackward0 object at 0x00000214026A26A0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JangChanYoung\\AppData\\Local\\Temp\\ipykernel_38456\\3004614393.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(out.grad)\n"
     ]
    }
   ],
   "source": [
    "print(out.data) \n",
    "print(out.grad) \n",
    "print(out.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이터치에서는 데이터준비를 위해 torch.utils.data의 Dataset과 DataLoader 사용가능  \n",
    " - Dataset에는 다양한 데이터셋이 존재 (ex MNIST)  \n",
    " - DataLoader와 Dataset을 통해 batch_size, train여부, transform등을 인자로 넣어 데이터를 어떻게 load할 것인지 정해줄 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토치비전(torchvision)은 파이토치에서 제공하는 데이터셋들이 모여있는 패키지  \n",
    " - transforms : 전처리할 떄 사용하는 메소드  \n",
    " - transforms : 에서 제공하는 클래스 이외는 일반적으로 클래스를 따로 만들어 전처리 단계 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
